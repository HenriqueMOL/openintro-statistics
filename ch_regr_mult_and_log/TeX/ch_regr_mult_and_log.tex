\chapter{Multiple and logistic regression}
\label{multipleRegressionAndANOVA}
\label{multipleAndLogisticRegression}
\label{ch_regr_mult_and_log}
\renewcommand{\chapterfolder}{ch_regr_mult_and_log}

The principles of simple linear regression lay the foundation for more sophisticated regression methods used in a wide range of challenging settings. In Chapter~\ref{multipleAndLogisticRegression}, we explore multiple regression, which introduces the possibility of more than one predictor, and logistic regression, a technique for predicting categorical outcomes with two possible categories.




\section{Introduction to multiple regression}
\label{introductionToMultipleRegression}

\index{multiple regression|(}

Multiple regression extends simple two-variable regression to the case that still has one response but many predictors (denoted $x_1$, $x_2$, $x_3$, ...). The method is motivated by scenarios where many variables may be simultaneously connected to an output.

\index{data!loans|(}

\newcommand{\loNcomma}{10,000}
\newcommand{\loN}{10000}

We will consider data about loans from the peer-to-peer lender,
Lending Club, which is a data set we first encountered in
Chapters~\ref{ch_intro_to_data}
and~\ref{ch_summarizing_data}.
The loan data includes terms of the loan as well as
information about the borrower.
The outcome variable we would like to better understand
is the interest rate assigned to the loan.
For instance, all other characteristics held constant,
does it matter how much debt someone already has?
Does it matter if their income has been verified?
Multiple regression will help us answer these and other questions.

The data set \data{loans} includes results from \loNcomma{} loans,
and we'll be looking at a subset of the available variables,
some of which will be new from those we saw in earlier chapters.
The first six observations in the data set are shown in
Figure~\ref{loansDataMatrix},
and descriptions for each variable are shown in
Figure~\ref{loansVariables}.
Notice that the past bankruptcy variable (\var{past\us{}bankr})
is an indicator variable\index{indicator variable},
where it takes the value 1 if the borrower had a past
bankruptcy in their record and 0 if not.
Using an indicator variable in place of a category name
allows for these variables to be directly used in regression.
Two of the other variables are
categorical\index{categorical variable}
(\var{ver\us{}income} and \var{issued}), each of which
can take one of a few different non-numerical values;
we'll discuss how these are handled in the model in
Section~\ref{ind_and_cat_vars_as_predictors}.

\begin{figure}[h]
\centering\footnotesize
\begin{tabular}{r ccc ccc cc}
  \hline
   & interest\us{}rate & ver\us{}income
       & debt\us{}to\us{}income & credit\us{}util
       & past\us{}bankr & term
       & issued & credit\us{}checks \\ 
  \hline
  1 & 14.07 & verified & 18.01 & 0.55 & 0 & 60 & Mar2018 & 6 \\ 
  2 & 12.61 & not & 5.04 & 0.15 & 1 & 36 & Feb2018 & 1 \\ 
  3 & 17.09 & source\_only & 21.15 & 0.66 & 0 & 36 & Feb2018 & 4 \\ 
  4 & 6.72 & not & 10.16 & 0.20 & 0 & 36 & Jan2018 & 0 \\ 
  5 & 14.07 & verified & 57.96 & 0.75 & 0 & 36 & Mar2018 & 7 \\ 
  6 & 6.72 & not & 6.46 & 0.09 & 0 & 36 & Jan2018 & 6 \\
  $\vdots$ & $\vdots$ & $\vdots$ &
      $\vdots$ & $\vdots$ & $\vdots$ &
      $\vdots$ & $\vdots$ & $\vdots$ \\
   \hline
\end{tabular}
\caption{First six rows from the \data{loans} data set.}
\label{loansDataMatrix}
\end{figure}
%library(openintro)  # Run some example code from loans_full_schema
%library(xtable); xtable(rbind.data.frame(head(d[, c("interest_rate", co)], 6))) #, tail(d[, c("interest_rate", co)], 2)))

\begin{figure}[h]
\centering\small
\begin{tabular}{lp{11.5cm}}
\hline
{\bf variable} & {\bf description} \\
\hline
\var{interest\us{}rate} &
    final auction price plus shipping costs, in US dollars \\
\var{ver\us{}income} &
    Categorical variable describing whether the borrower's
    income source and amount have been verified. \\
\var{debt\us{}to\us{}income} &
    Debt-to-income ratio, which is the ratio of total debt
    of the borrower divided by their total income. \\
\var{credit\us{}util} &
    Of all the credit available to the borrower,
    what fraction are they utilizing.
    For example, the credit utilization on a credit card would
    be the card's balance divided by the card's credit limit. \\
\var{past\us{}bankr} &
    An indicator variable for whether the borrower has a past
    bankruptcy in her record. This variable takes a value of
    \resp{1} if the answer is ``yes''
    and \resp{0} if the answer is ``no''. \\
\var{term} &
    The length of the loan, in months. \\
\var{issued} &
    The month and year the loan was issued,
    which for these loans is always during the first
    quarter of 2018. \\
\var{credit\us{}checks} &
    Number of credit checks in the last 12 months.
    For example, when filing an application for a credit card,
    it is common for the company receiving the application
    to run a credit check. \\
\hline
\end{tabular}
\caption{Variables and their descriptions for the \data{mario\us{}kart} data set.}
\label{loansVariables}
\end{figure}


\newpage

\subsection{Indicator and categorical variables as predictors}
\label{ind_and_cat_vars_as_predictors}

\newcommand{\pastbankrACoef}{0.74}
\newcommand{\pastbankrACoefSE}{0.15}

Let's start by fitting a linear regression model for
interest rate with a single predictor indicating whether
or not a person has a bankruptcy in their record:
\begin{align*}
\widehat{rate} &= 12.33 + \pastbankrACoef{} \times past\us{}bankr
\end{align*}
Results of this model are shown in
Figure~\ref{intRateVsPastBankrModel}.
%and a scatterplot for price
%versus game condition is shown in
%Figure~\ref{intRateVsPastBankrScatter}.

\begin{figure}[h]
\centering
\begin{tabular}{l rrr r}
  \hline
  \vspace{-3.7mm} & & & & \\
  & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  (Intercept) & 12.3380 & 0.0533 & 231.49 & $<$0.0001 \\ 
  past\us{}bankr & 0.7368 & 0.1529 & 4.82 & $<$0.0001 \\ 
  \hline
  &&&\multicolumn{2}{r}{$df=9998$}
\end{tabular}
\caption{Summary of a linear model for predicting
    interest rate based on whether the borrower has
    a bankruptcy in their record.}
\label{intRateVsPastBankrModel}
\end{figure}

%\begin{figure}[h]
%  \centering
%  \Figures{0.45}{loansSingles}{intRateVsPastBankrScatter}
%  \caption{Scatterplot of interest rate against
%      the past bankruptcy indicator variable.
%      The least squares line is also shown,
%      representing a relatively small difference
%      between the two bankruptcy groups.}
%  \label{intRateVsPastBankrScatter}
%\end{figure}

%\begin{exercisewrap}
%\begin{nexercise}
%Examine Figure~\ref{intRateVsPastBankrScatter}.
%Are the conditions for a linear model reasonable?\footnotemark
%\end{nexercise}
%\end{exercisewrap}
%\footnotetext{Yes. Constant variability, nearly normal residuals, and linearity all appear reasonable.}

\begin{examplewrap}
\begin{nexample}{Interpret the coefficient for the
     past bankruptcy variable in the model.
     Is this coefficient significantly different from 0?}
  The \var{past\us{}bankr} variable is a two-level categorical
  variable, taking value 1 when the borrower has a bankruptcy
  in their history and 0 otherwise.
  So \pastbankrACoef{} means that the model predicts a
  \pastbankrACoef{}\% higher
  interest rate for those borrowers with a bankruptcy in
  their record.
  (See Section~\ref{categoricalPredictorsWithTwoLevels}
  for a review of the interpretation for two-level
  categorical predictor variables.)
  Examining the regression output in
  Figure~\ref{intRateVsPastBankrModel},
  we can see that the p-value for \var{past\us{}bankr}
  is very close to zero, indicating there is strong evidence
  that the coefficient is different from zero when using this
  simple one-variable model.
\end{nexample}
\end{examplewrap}

Suppose we had fit a model using a 3-level categorical variable,
such as \var{ver\us{}income}.
The output from software is shown in
Figure~\ref{intRateVsVerIncomeModel}.
This regression output provides multiple
rows for the \var{ver\us{}income} variable.
Each row represents the relative difference for
each level of \var{ver\us{}income}.
However, we are missing one of the levels:
\resp{not} (for \emph{not verified}).
The missing level is called the \term{reference level},
and it represents the default level that
other levels are measured against.
%This will make more sense after we write out the equation.

\begin{figure}[h]
\centering
\begin{tabular}{l rrr r}
  \hline
  \vspace{-3.7mm} & & & & \\
  & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  (Intercept) &
      11.0995 & 0.0809 & 137.18 & $<$0.0001 \\
  ver\us{}income\lmlevel{source\us{}only} &
      1.4160 & 0.1107 & 12.79 & $<$0.0001 \\ 
  ver\us{}income\lmlevel{verified} &
      3.2543 & 0.1297 & 25.09 & $<$0.0001 \\ 
  \hline
  &&&\multicolumn{2}{r}{$df=9998$}
\end{tabular}
\caption{Summary of a linear model for predicting
    interest rate based on whether the borrower's
    income source and amount has been verified.
    This predictor has three levels, which results
    in 2 rows in the regression output.}
\label{intRateVsVerIncomeModel}
\end{figure}

\begin{examplewrap}
\begin{nexample}{How would we write an equation for
    this regression model?}
  \label{verIncomeEquationExample}%
  The equation for the linear model may be written as
  a model with two predictors:
  \begin{align*}
  \widehat{rate} = 11.10 +
      1.42 \times
          %1_{\var{ver\us{}income} = \resp{source\us{}only}} +
          \indfunc{ver\us{}income}{source\us{}only} +
      3.25 \times
          %1_{\var{ver\us{}income} = \resp{verified}}
          \indfunc{ver\us{}income}{verified}
  \end{align*}
  We use the notation $\indfunc{variable}{level}$
  to represent \termsub{indicator variables}{indicator variable}
  for when the categorical variable takes a particular value.
  For example, $\indfunc{ver\us{}income}{source\us{}only}$
  would take a value of 1 if \var{ver\us{}income} was
  \resp{source\us{}only} for a loan,
  and it would take a value of 0 otherwise.
  Likewise, $\indfunc{ver\us{}income}{verified}$ would take
  a value of 1 if \var{ver\us{}income} took a value
  of \resp{verified} and 0 if it took any other value.
  % In Example~\ref{}, we'll run through a few examples
  % of how we can use the equation for the model.
\end{nexample}
\end{examplewrap}

The notation used in Example~\ref{verIncomeEquationExample}
may feel a bit confusing.
Let's figure out how to use the equation for each level
of the \var{ver\us{}income} variable.

\begin{examplewrap}
\begin{nexample}{Using the model from
    Example~\ref{verIncomeEquationExample},
    compute the average interest rate for borrowers
    whose income source and amount are both unverified.}
  When \var{ver\us{}income} takes a value of \resp{not},
  then both indicator functions in the equation from
  Example~\ref{verIncomeEquationExample}
  are set to zero:
  \begin{align*}
  \widehat{rate} &= 11.10 +
      1.42 \times 0 +
      3.25 \times 0 \\
    &= 11.10
  \end{align*}
  The average interest rate for these borrowers is 11.1\%.
  Because the \resp{not} level does not have its own
  coefficient and it is the reference value,
  the indicators for the other levels for this variable
  all drop out.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{Using the model from
    Example~\ref{verIncomeEquationExample},
    compute the average interest rate for borrowers
    whose income source is verified but the amount is not.}
  When \var{ver\us{}income} takes a value of
  \resp{source\us{}only},
  then the corresponding indicator function is 1
  while the other ($1_{verified}$) is 0:
  \begin{align*}
  \widehat{rate} &= 11.10 +
      1.42 \times 1 +
      3.25 \times 0 \\
    &= 12.52
  \end{align*}
  The average interest rate for these borrowers is 12.52\%.
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}
Compute the average interest rate for borrowers
whose income source and amount are both verified.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{When \var{ver\us{}income} takes a value of
  \resp{verified},
  then the corresponding indicator function is 1
  while the other ($1_{source\_only}$) is 0:
  \begin{align*}
  \widehat{rate} &= 11.10 +
      1.42 \times 0 +
      3.25 \times 1 \\
    &= 14.35
  \end{align*}
  The average interest rate for these borrowers is 14.35\%.}

\begin{onebox}{Predictors with several categories}
When fitting a regression model with a categorical variable
that has $k$ levels where $k > 2$, software will provide
a coefficient for $k - 1$ of those levels.
For the last level that does not receive a coefficient,
this is the \term{reference level}, and the coefficients
listed for the other levels are all considered relative
to this reference level.
\end{onebox}

\begin{exercisewrap}
\begin{nexercise}
Interpret the coefficients in the \var{ver\us{}income}
model.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Each of the coefficients gives the
  incremental interest rate for the corresponding level
  relative to the \resp{not} level, which is the reference
  level.
  For example, for a borrower whose income source and
  amount have been verified, the model predicts that
  they will have a 3.25\% higher interest rate than
  a borrower who has not had their income source or
  amount verified.}

The higher interest rate for borrowers who have verified
their income source or amount is surprising.
Intuitively, we'd think that a loan would look \emph{less}
risky if the borrower's income has been verified.
However, note that the situation may be more complex,
and there may be confounding variables.
For example, perhaps lender require borrowers with
poor credit to verify their income.
That is, verifying income in our data set might be
a signal of some concerns about the borrower
rather than a reassurance that the borrower will pay
back the loan.
For this reason, the borrower could be deemed higher
risk, resulting in a higher interest rate.
(What other confounding variables might explain this
counter-intuitive model?)

\begin{exercisewrap}
\begin{nexercise}
How much larger of an interest rate would we expect for
a borrower who has verified their income source and amount
vs a borrower whose income source has only been
verified?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Relative to the \resp{not} category,
  the \resp{verified} category has an interest rate of
  3.25\% higher, while the \resp{source\us{}only}
  category is only 1.42\% higher.
  Thus, \resp{verified} borrowers will tend to get
  an interest rate about $3.25\% - 1.42\% = 1.83\%$
  higher than \resp{source\us{}only} borrowers.}


\subsection{Including and assessing many variables in a model}
\label{includingAndAssessingManyVariablesInAModel}

The world is complex, and it can be helpful to be able to
consider many factors at once in statistical modeling.
For example, we might like to use the full context of
borrower to predict the interest rate they receive
rather than using a single variable.
This is the strategy used in \term{multiple regression}.
While we remain cautious about making any causal
interpretations using multiple regression,
such models are a common first step in gaining insights
or providing some evidence of a causal connection.

We want to construct a model that accounts for not only
for any past bankruptcy or whether the borrower had
their income source or amount verified,
but simultaneously accounts for all the variables
in the data set:
\var{ver\us{}income},
\var{debt\us{}to\us{}income},
\var{credit\us{}util},
\var{past\us{}bankr},
\var{term},
\var{issued},
and \var{credit\us{}checks}.
\begin{align*}
\widehat{\var{rate}}
	&= \beta_0 +
	    \beta_1\times \indfunc{ver\us{}income}{source\us{}only} +
	    \beta_2\times \indfunc{ver\us{}income}{verified} +
		\beta_3\times \var{debt\us{}to\us{}income} \\
	&\qquad\  +
	    \beta_4 \times \var{credit\us{}util} +
	    \beta_5 \times \var{past\us{}bankr} +
		\beta_6 \times \var{term} \\
	&\qquad\  +
	    \beta_7 \times \indfunc{issued}{Jan2018} +
	    \beta_8 \times \indfunc{issued}{Mar2018} +
		\beta_9 \times \var{credit\us{}checks}
\end{align*}
This equation represents a holistic approach for modeling
all of the variables simultaneously.
Notice that there are two coefficients for \var{ver\us{}income}
and also two coefficients for \var{issued}, since both are
3-level categorical variables.

%\Comment{Work on this paragraph.}
%A multiple regression model may be missing important components or it might not precisely represent the relationship between the outcome and the available explanatory variables. While no model is perfect, we wish to explore the possibility that this one may fit the data reasonably well.


We estimate the parameters
$\beta_0$, $\beta_1$, $\beta_2$, ..., $\beta_9$
in the same way as we did in the case of a single predictor.
We select $b_0$, $b_1$, $b_2$, ..., $b_9$ that minimize the
sum of the squared residuals:
\begin{align}\label{sumOfSqResInMultRegr}
SSE = e_1^2 + e_2^2 + \dots + e_{\loN}^2
	= \sum_{i=1}^{\loN} e_i^2
	 = \sum_{i=1}^{\loN} \left(y_i - \hat{y}_i\right)^2
\end{align}
where $y_i$ and $\hat{y}_i$ represent the observed
interest rates and their estimated values according to
the model, respectively.
There are \loNcomma{} residuals, one for each observation.
We typically use a computer to minimize the sum of squares
and compute point estimates, as shown in the sample output
in Figure~\ref{outputForMultipleRegrOutputForAllPredictors}.
Using this output, we identify the point estimates $b_i$ of
each $\beta_i$, just as we did in the one-predictor case.

\newcommand{\pastbankrFullCoef}{0.39}
\newcommand{\pastbankrFullCoefSE}{0.13}

\begin{figure}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
  \vspace{-3.7mm} & & & & \\
  & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  \vspace{-3.8mm} & & & & \\
  (Intercept) & 1.9251 & 0.2102 & 9.16 & $<$0.0001 \\ 
  ver\us{}income\lmlevel{source\us{}only} &
      0.9750 & 0.0991 & 9.83 & $<$0.0001 \\ 
  ver\us{}income\lmlevel{verified} &
      2.5374 & 0.1172 & 21.65 & $<$0.0001 \\ 
  debt\us{}to\us{}income & 0.0211 & 0.0029 & 7.18 & $<$0.0001 \\ 
  credit\us{}util & 4.8959 & 0.1619 & 30.24 & $<$0.0001 \\ 
  past\us{}bankr & 0.3864 & 0.1324 & 2.92 & 0.0035 \\ 
  term & 0.1537 & 0.0039 & 38.96 & $<$0.0001 \\ 
  issued\lmlevel{Jan2018} & 0.0276 & 0.1081 & 0.26 & 0.7981 \\ 
  issued\lmlevel{Mar2018} & -0.0397 & 0.1065 & -0.37 & 0.7093 \\ 
  credit\us{}checks & 0.2282 & 0.0182 & 12.51 & $<$0.0001 \\ 
   \hline
   &&&\multicolumn{2}{r}{$df=9990$}
\end{tabular}
\caption{Output for the regression model where
    \var{interest\us{}rate} is the outcome and
    variables listed are the predictors.}
\label{outputForMultipleRegrOutputForAllPredictors}
\end{figure}

\begin{onebox}{Multiple regression model}
  A multiple regression model is a linear model
  with many predictors.
  In general, we write the model as
  \begin{align*}
  \hat{y} =
      \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k
  \end{align*}
  when there are $k$ predictors.
  We always estimate the $\beta_i$ parameters using
  statistical software.
\end{onebox}

\begin{examplewrap}
\begin{nexample}{Write out the regression model using
    the point estimates from
    Figure~\ref{outputForMultipleRegrOutputForAllPredictors}.
    How many predictors are there in this model?}
  \label{loansFullModelEqWCoef}%
  The fitted model for the interest rate is given by:
  {\small\begin{align*}
  \widehat{\var{rate}}
	&= 1.925 +
	    0.975 \times \indfunc{ver\us{}income}{source\us{}only} +
	    2.537 \times \indfunc{ver\us{}income}{verified} +
		0.021 \times \var{debt\us{}to\us{}income} \\
	&\qquad\  +
	    4.896 \times \var{credit\us{}util} +
	    0.386 \times \var{past\us{}bankr} +
		0.154 \times \var{term} \\
	&\qquad\  +
	    0.028 \times \indfunc{issued}{Jan2018}
	    -0.040 \times \indfunc{issued}{Mar2018} +
		0.228 \times \var{credit\us{}checks}
  \end{align*}}
\end{nexample}
\end{examplewrap}


\begin{exercisewrap}
\begin{nexercise}
What does $\beta_4$, the coefficient of variable
\var{credit\us{}util}, represent?
What is the point estimate of~$\beta_4$?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{$\beta_4$ represents the change in
   interest rate we would expect if someone's credit
   utilization was 0 and went to 1,
   all other factors held even.
   The point estimate is $b_4 = 4.90\%$.}

\begin{examplewrap}
\begin{nexample}{Compute the residual of the first observation
    in Table~\ref{loansDataMatrix} on
    page~\pageref{loansDataMatrix} using the equation identified
    in Guided Practice~\ref{loansFullModelEqWCoef}.}
  To compute the residual, we first need the predicted value,
  which we compute by plugging values into the equation from
  Example~\ref{loansFullModelEqWCoef}.
  For example, $\indfunc{ver\us{}income}{source\us{}only}$
  takes a value of 0,
  $\indfunc{ver\us{}income}{verified}$ takes a value of 1
  (since the borrower's income source and amount were verified),
  \var{debt\us{}to\us{}income} was 18.01, and so on.
  This leads to a prediction of $\widehat{rate}_1 = 18.09$.
  The observed interest rate was 14.07\%, which leads to
  a residual of $e_1 = 14.07 - 18.09 = -4.02$.
\end{nexample}
\end{examplewrap}
% sum(model.matrix(m)[1, ] * round(m$coef, 3))

\begin{examplewrap}
\begin{nexample}{We estimated a coefficient for
    \var{past\us{}bankr} in
    Section~\ref{ind_and_cat_vars_as_predictors}
    of $b_4 = \pastbankrACoef{}$ with a standard error
    of $SE_{b_1} = \pastbankrACoefSE{}$ when using simple
    linear regression.
    Why is there be a difference between that estimate
    and the estimated coefficient of \pastbankrFullCoef{}
    in the multiple regression setting?}
  \label{pastBankrCoefDiffExplained}%
  If we examined the data carefully, we would see that
  some predictors are correlated.
  For instance, when we estimated the connection of the
  outcome \var{interest\us{}rate} and predictor
  \var{past\us{}bankr} using simple linear regression,
  we were unable to control for other variables like
  whether the borrower had her income verified,
  the borrower's debt-to-income ratio, and other variables.
  That original model was constructed in a vacuum and did
  not consider the full context.
  When we all of the variables, underlying and unintentional
  bias that was missed by these other variables is reduced
  or eliminated.
  Of course, bias can still exist from other confounding
  variables.
\end{nexample}
\end{examplewrap}

Example~\ref{pastBankrCoefDiffExplained} describes a common
issue in multiple regression: correlation among predictor
variables.
We say the two predictor variables are \term{collinear}
(pronounced as \emph{co-linear}) when they are correlated,
and this collinearity complicates model estimation.
While it is impossible to prevent collinearity from arising
in observational data, experiments are usually designed to
prevent predictors from being collinear.

\begin{exercisewrap}
\begin{nexercise}
The estimated value of the intercept is 1.925, and one might
be tempted to make some interpretation of this coefficient,
such as, it is the model's predicted price when each of the
variables take value zero: income source is not verified,
the borrower has no debt (debt-to-income and credit
utilization are zero), and so on.
Is this reasonable?
Is there any value gained by making this
interpretation?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{Many of the variables do take a value 0
   for at least one data point, and for those variables,
   it is reasonable.
   However, one variable never takes a value of zero:
   \var{term}, which describes the length of the loan,
   in months.
   If \var{term} is set to zero, then the loan
   must be paid back immediately; the borrower
   must give the money back as soon as she receives it,
   which means it is not a real loan.
   Ultimately, the interpretation of the intercept in
   this setting is not insightful.}


\subsection{Adjusted $R^2$ as a better estimate
    of explained variance}

\index{adjusted r squared@adjusted $R^2$ ($R_{adj}^2$)|(}

We first used $R^2$ in Section~\ref{fittingALineByLSR}
to determine the amount of variability in the response
that was explained by the model:
\begin{align*}
R^2 =
    1 - \frac{\text{variability in residuals}}
        {\text{variability in the outcome}}
	= 1 - \frac{Var(e_i)}{Var(y_i)}
\end{align*}
where $e_i$ represents the residuals of the model and
$y_i$ the outcomes.
This equation remains valid in the multiple regression
framework, but a small enhancement can make it even
more informative.

\begin{exercisewrap}
\begin{nexercise}
\label{computeUnadjR2ForFullLoansModel}%
The variance of the residuals for the model given in
Guided Practice~\ref{loansFullModelEqWCoef}
is 18.53, and the variance of the total price in all
the auctions is 25.01.
Calculate $R^2$ for this model.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{$R^2 = 1 - \frac{18.53}{25.01} = 0.2591$.}

This strategy for estimating $R^2$ is acceptable when there
is just a single variable.
However, it becomes less helpful when there are many
variables.
The regular $R^2$ is a less estimate of the amount of
variability explained by the model.
To get a better estimate, we use the adjusted $R^2$.

\begin{onebox}{Adjusted $\mathbf{R^2}$ as a tool for
    model assessment}
  The \termsub{adjusted $\mathbf{R^2}$}
      {adjusted r squared@adjusted $R^2$ ($R_{adj}^2$)}
  is computed as
  \begin{align*}
  R_{adj}^{2} = 1-\frac{Var(e_i) / (n-k-1)}{Var(y_i) / (n-1)}
      = 1-\frac{Var(e_i)}{Var(y_i)} \times \frac{n-1}{n-k-1}
  \end{align*}
  where $n$ is the number of cases used to fit the model
  and $k$ is the number of predictor variables in the model.
\end{onebox}

Because $k$ is never negative, the adjusted $R^2$ will be
smaller -- often times just a little smaller -- than the
unadjusted $R^2$.
The reasoning behind the adjusted $R^2$ lies in the
\termsub{degrees of freedom}{degrees of freedom (df)!regression}
associated with each variance,
which is equal to $n - k - 1$ for the multiple regression
context.
If we were to make predictions for new data
using our current model, we would find that the unadjusted
$R^2$ would tend to be slightly overly optimistic, while
the adjusted $R^2$ formula helps correct this bias.

\begin{exercisewrap}
\begin{nexercise}
There were $n=10000$ auctions in the \data{loans} data set
and $k=9$ predictor variables in the model.
Use $n$, $k$, and the variances from
Guided Practice~\ref{computeUnadjR2ForFullLoansModel}
to calculate $R_{adj}^2$ for the interest rate
model.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{$R_{adj}^2
    = 1 - \frac{18.53}{25.01}\times \frac{10000-1}{1000-9-1}
    = 0.2584$.
  While the difference is very small, it will be important
  when we fine tune the model in the next section.}

\begin{exercisewrap}
\begin{nexercise}
Suppose you added another predictor to the model, but the
variance of the errors $Var(e_i)$ didn't go down.
What would happen to the~$R^2$?
What would happen to the
adjusted~$R^2$?\hspace{0.7mm}\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{The unadjusted $R^2$ would stay the same
    and the adjusted $R^2$ would go down.}

Adjusted $R^2$ could have been used in
Chapter~\ref{linRegrForTwoVar}.
However, when there is only $k = 1$ predictors,
adjusted $R^2$ is very close to regular $R^2$,
so this nuance isn't typically important when
considering only one predictor.

\index{adjusted r squared@adjusted $R^2$ ($R_{adj}^2$)|)}





%__________________
\section{Model selection}
\label{model_selection_section}
\label{modelSelection}

\index{model selection|(}

The best model is not always the most complicated.
Sometimes including variables that are not evidently
important can actually reduce the accuracy of predictions.
In this section we discuss model selection strategies,
which will help us eliminate variables from the model that
are found to be less important.

In practice, the model that includes all available explanatory
variables is often referred to as the \term{full model}.
The full model may not be the best model, and if it isn't,
we want to identify a smaller model that is preferable.


\subsection{Identifying variables in the model that may
    not be helpful}

Adjusted $R^2$ describes the strength of a model fit,
and it is a useful tool for evaluating which predictors
are adding value to the model, where \emph{adding value}
means they are (likely) improving the accuracy in
predicting future outcomes.

Let's consider two models, which are shown in
Tables~\ref{outputForMultipleRegrOutputForAllPredictors2}
and~\ref{marioKartMultipleRegressionModelAllButDuration}.
The first table summarizes the full model since it includes
all predictors, while the second does not include the
\var{issued} variable.

\begin{figure}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
  \vspace{-3.7mm} & & & & \\
  & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  \vspace{-3.8mm} & & & & \\
  (Intercept) & 1.9251 & 0.2102 & 9.16 & $<$0.0001 \\ 
  ver\us{}income\lmlevel{source\us{}only} &
      0.9750 & 0.0991 & 9.83 & $<$0.0001 \\ 
  ver\us{}income\lmlevel{verified} &
      2.5374 & 0.1172 & 21.65 & $<$0.0001 \\ 
  debt\us{}to\us{}income & 0.0211 & 0.0029 & 7.18 & $<$0.0001 \\ 
  credit\us{}util & 4.8959 & 0.1619 & 30.24 & $<$0.0001 \\ 
  past\us{}bankr & 0.3864 & 0.1324 & 2.92 & 0.0035 \\ 
  term & 0.1537 & 0.0039 & 38.96 & $<$0.0001 \\ 
  issued\lmlevel{Jan2018} & 0.0276 & 0.1081 & 0.26 & 0.7981 \\ 
  issued\lmlevel{Mar2018} & -0.0397 & 0.1065 & -0.37 & 0.7093 \\ 
  credit\us{}checks & 0.2282 & 0.0182 & 12.51 & $<$0.0001 \\ 
  \hline
  \multicolumn{3}{l}{$R_{adj}^2 = 0.25843$}&
      \multicolumn{2}{r}{$df=9990$}
\end{tabular}
\caption{The fit for the full regression model,
    including the adjusted $R^2$.}
\label{outputForMultipleRegrOutputForAllPredictors2}
\end{figure}

\begin{figure}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
  \vspace{-3.7mm} & & & & \\
  & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
  \vspace{-3.8mm} & & & & \\
  (Intercept) & 1.9213 & 0.1982 & 9.69 & $<$0.0001 \\ 
  ver\us{}income\lmlevel{source\us{}only} &
      0.9740 & 0.0991 & 9.83 & $<$0.0001 \\ 
  ver\us{}income\lmlevel{verified} &
      2.5355 & 0.1172 & 21.64 & $<$0.0001 \\ 
  debt\us{}to\us{}income & 0.0211 & 0.0029 & 7.19 & $<$0.0001 \\ 
  credit\us{}util & 4.8958 & 0.1619 & 30.25 & $<$0.0001 \\ 
  past\us{}bankr & 0.3869 & 0.1324 & 2.92 & 0.0035 \\ 
  term & 0.1537 & 0.0039 & 38.97 & $<$0.0001 \\ 
  credit\us{}checks & 0.2283 & 0.0182 & 12.51 & $<$0.0001 \\ 
  \hline
  \vspace{-3.6mm} & & & & \\
  \multicolumn{3}{l}{$R_{adj}^2 = 0.25854$}&
      \multicolumn{2}{r}{$df=9992$}
\end{tabular}
\caption{The fit for the regression model after dropping
   the \var{issued} variable.} %, which represented 3 categories
   % and 2 degrees of freedom.}
\label{marioKartMultipleRegressionModelAllButDuration}
\end{figure}

\begin{examplewrap}
\begin{nexample}{Which of the two models is better?}
  We compare the adjusted $R^2$ of each model to determine
  which to choose.
  Since the first model has an $R^2_{adj}$ smaller than
  the $R^2_{adj}$ of the second model, we prefer the second
  model to the first.
\end{nexample}
\end{examplewrap}

Will the model without \var{issued} be better than the
model with \var{issued}?
We~cannot know for sure, but based on the adjusted $R^2$,
this is our best assessment.


\subsection{Two model selection strategies}

Two common strategies for adding or removing variables in a multiple regression model are called \emph{backward elimination} and \emph{forward selection}. These techniques are often referred to as \term{stepwise} model selection strategies, because they add or delete one variable at a time as they ``step'' through the candidate predictors.

\termsub{Backward elimination}{backward elimination} starts with the model that includes all potential predictor variables. Variables are eliminated one-at-a-time from the model until we cannot improve the adjusted $R^2$. The strategy within each elimination step is to eliminate the variable that leads to the largest improvement in adjusted $R^2$.

\begin{examplewrap}
\begin{nexample}{Results corresponding to the \emph{full model} for the \data{mario\us{}kart} data are shown in Figure~\ref{outputForMultipleRegrOutputForAllPredictors2}. How should we proceed under the backward elimination strategy?} \label{backwardEliminationExampleWMarioKartData}
Our baseline adjusted $R^2$ from the full model is $R^2_{adj} = 0.7108$, and we need to determine whether dropping a predictor will improve the adjusted $R^2$. To check, we fit four models that each drop a different predictor, and we record the adjusted $R^2$ from each:
\begin{center}
\begin{tabular}{lllll}
Exclude ... &
	\var{cond\us{}new} &
	\var{stock\us{}photo} &
	\var{duration} &
	\var{wheels} \\
&
	$R^2_{adj} = 0.6626$ &
	$R^2_{adj} = 0.7107$ &
	$R^2_{adj} = 0.7128$ &
	$R^2_{adj} = 0.3487$ \\
\end{tabular}
\end{center}
The third model without \var{duration} has the highest adjusted $R^2$ of 0.7128, so we compare it to the adjusted $R^2$ for the full model. Because eliminating \var{duration} leads to a model with a higher adjusted $R^2$, we drop \var{duration} from the model.

Since we eliminated a predictor from the model in the first step, we see whether we should eliminate any additional predictors. Our baseline adjusted $R^2$ is now $R^2_{adj} = 0.7128$. We now fit three new models, which consider eliminating each of the three remaining predictors:
\begin{center}
\begin{tabular}{llll}
Exclude \var{duration} and ... &
	\var{cond\us{}new} &
	\var{stock\us{}photo} &
	\var{wheels} \\
&
	$R^2_{adj} = 0.6587$ &
	$R^2_{adj} = 0.7124$ &
	$R^2_{adj} = 0.3414$ \\
\end{tabular}
\end{center}
None of these models lead to an improvement in adjusted $R^2$, so we do not eliminate any of the remaining predictors. That is, after backward elimination, we are left with the model that keeps \var{cond\us{}new}, \var{stock\us{}photos}, and \var{wheels}, which we can summarize using the coefficients from Figure~\ref{marioKartMultipleRegressionModelAllButDuration}:
\begin{align*}
\hat{y} \ &= \ b_0 + b_1x_1 + b_2x_2 + b_4x_4 \\
\widehat{price} &= \ 36.05 + 5.18 \times \text{\var{cond\us{}new}} + 1.12 \times \text{\var{stock\us{}photo}} + 7.30 \times \text{\var{wheels}}
\end{align*}
\end{nexample}
\end{examplewrap}

The \term{forward selection} strategy is the reverse of the backward elimination technique. Instead of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model (as measured by adjusted $R^2$).

\begin{examplewrap}
\begin{nexample}{Construct a model for the \data{mario\us{}kart} data set using the forward selection strategy.}\label{forwardEliminationExampleWMarioKartData}
We start with the model that includes no variables. Then we fit each of the possible models with just one variable. That is, we fit the model including just \var{cond\us{}new}, then the model including just \var{stock\us{}photo}, then a model with just \var{duration}, and a model with just \var{wheels}. Each of the four models provides an adjusted $R^2$ value:
\begin{center}
\begin{tabular}{lllll}
Add ... &
	\var{cond\us{}new} &
	\var{stock\us{}photo} &
	\var{duration} &
	\var{wheels} \\
&
	$R^2_{adj} = 0.3459$ &
	$R^2_{adj} = 0.0332$ &
	$R^2_{adj} = 0.1338$ &
	$R^2_{adj} = 0.6390$ \\
\end{tabular}
\end{center}
In this first step, we compare the adjusted $R^2$ against a baseline model that has no predictors. The no-predictors model always has $R_{adj}^2 = 0$. The model with one predictor that has the largest adjusted $R^2$ is the model with the \var{wheels} predictor, and because this adjusted $R^2$ is larger than the adjusted $R^2$ from the model with no predictors ($R_{adj}^2 = 0$), we will add this variable to our model.

We repeat the process again, this time considering 2-predictor models where one of the predictors is \var{wheels} and with a new baseline of $R^2_{adj} = 0.6390$:
\begin{center}
\begin{tabular}{llll}
Add \var{wheels} and ... &
	\var{cond\us{}new} &
	\var{stock\us{}photo} &
	\var{duration} \\
&
	$R^2_{adj} = 0.7124$ &
	$R^2_{adj} = 0.6587$ &
	$R^2_{adj} = 0.6528$ \\
\end{tabular}
\end{center}
The best predictor in this stage, \var{cond\us{}new}, has a higher adjusted $R^2$ (0.7124) than the baseline (0.6390), so we also add \var{cond\us{}new} to the model.

Since we have again added a variable to the model, we continue and see whether it would be beneficial to add a third variable:
\begin{center}
\begin{tabular}{lll}
Add \var{wheels}, \var{cond\us{}new}, and ... &
	\var{stock\us{}photo} &
	\var{duration} \\
&
	$R^2_{adj} = 0.7128$ &
	$R^2_{adj} = 0.7107$ \\
\end{tabular}
\end{center}
The model adding \var{stock\us{}photo} improved adjusted $R^2$ (0.7124 to 0.7128), so we add \var{stock\us{}photo} to the model.

Because we have again added a predictor, we check whether adding the last variable, \var{duration}, will improve adjusted $R^2$. We compare the adjusted $R^2$ for the model with \var{duration} and the other three predictors (0.7108) to the model that only considers \var{wheels}, \var{cond\us{}new}, and \var{stock\us{}photo} (0.7128). Adding \var{duration} does not improve the adjusted $R^2$, so we do not add it to the model, and we have arrived at the same model that we identified from backward elimination.

\end{nexample}
\end{examplewrap}

\begin{onebox}{Model selection strategies}
Backward elimination begins with the largest model and eliminates variables one-by-one until we are satisfied that all remaining variables are important to the model. Forward selection starts with no variables included in the model, then it adds in variables according to their importance until no other important variables are found.
\end{onebox}

There is no guarantee that backward elimination and forward selection will arrive at the same final model. If both techniques are tried and they arrive at different models, we choose the model with the larger $R_{adj}^2$; other tie-break options exist but are beyond the scope of this book.



\subsection{The p-value approach, an alternative to adjusted $R^2$}

The p-value may be used as an alternative to adjusted $R^2$ for model selection.

In backward elimination, we would identify the predictor corresponding to the largest p-value. If the p-value is above the significance level, usually $\alpha = 0.05$, then we would drop that variable, refit the model, and repeat the process. If the largest p-value is less than $\alpha = 0.05$, then we would not eliminate any predictors and the current model would be our best-fitting model.

In forward selection with p-values, we reverse the process. We begin with a model that has no predictors, then we fit a model for each possible predictor, identifying the model where the corresponding predictor's p-value is smallest. If that p-value is smaller than $\alpha = 0.05$, we add it to the model and repeat the process, considering whether to add more variables one-at-a-time. When none of the remaining predictors can be added to the model and have a p-value less than 0.05, then we stop adding variables and the current model would be our best-fitting model.

\begin{exercisewrap}
\begin{nexercise}
Examine Figure~\ref{marioKartMultipleRegressionModelAllButDuration} on page~\pageref{marioKartMultipleRegressionModelAllButDuration}, which considers the model including the \var{cond\us{}new}, \var{stock\us{}photo}, and \var{wheels} predictors. If we were using the p-value approach with backward elimination and we were considering this model, which of these three variables would be up for elimination? Would we drop that variable, or would we keep it in the model?\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{The \var{stock\us{}photo} predictor is up for elimination since it has the largest p-value. Additionally, since that p-value is larger than 0.05, we would in fact eliminate \var{stock\us{}photo} from the model.}

While the adjusted $R^2$ and p-value approaches are similar, they sometimes lead to different models, with the adjusted $R^2$ approach tending to include more predictors in the final model. For example, if we had used the p-value approach with the auction data, we would not have included the \var{stock\us{}photo} predictor in the final model.

\begin{onebox}{Adjusted $\mathbf{R^2}$ vs p-value approach}
When the sole goal is to improve prediction accuracy, use adjusted $R^2$. This is commonly the case in machine learning applications.\vspace{3mm}

When we care about understanding which variables are statistically significant predictors of the response, or if there is interest in producing a simpler model at the potential cost of a little prediction accuracy, then the p-value approach is preferred.
\end{onebox}

Regardless of whether you use adjusted $R^2$ or the p-value approach, or if you use the backward elimination of forward selection strategy, our job is not done after variable selection. We must still verify the model conditions are reasonable.

\index{model selection|)}



%%%%%
\section{Checking model assumptions using graphs}
\label{multipleRegressionModelAssumptions}

\index{multiple regression!model assumptions|(}

Multiple regression methods using the model
\begin{align*}
\hat{y} &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k
\end{align*}
generally depend on the following four assumptions:
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item the residuals of the model are nearly normal,
\item the variability of the residuals is nearly constant,
\item the residuals are independent, and
\item each variable is linearly related to the outcome.
\end{enumerate}
\termsub{Diagnostic plots}{diagnostic plots} can be used to check each of these assumptions. We will consider the model from the Mario Kart auction data, and check whether there are any notable concerns:
\begin{align*}
\widehat{price} &= \ 36.05 + 5.18 \times \text{\var{cond\us{}new}} + 1.12 \times \text{\var{stock\us{}photo}} + 7.30 \times \text{\var{wheels}}
\end{align*}

\begin{description}
\item[Normal probability plot.] A normal probability plot of the residuals is shown in Figure~\ref{mkDiagnosticNormalQuantilePlot}. While the plot exhibits some minor irregularities, there are no outliers that might be cause for concern. In a normal probability plot for residuals, we tend to be most worried about residuals that appear to be outliers, since these indicate long tails in the distribution of residuals.

\begin{figure}[h]
  \centering
  \Figures{0.71}
      {marioKartDiagnostics}
      {mkDiagnosticNormalQuantilePlot}
  \caption{A normal probability plot of the residuals is helpful in identifying observations that might be outliers.}
  \label{mkDiagnosticNormalQuantilePlot}
\end{figure}

\item[Absolute values of residuals against fitted values.] A plot of the absolute value of the residuals against their corresponding fitted values ($\hat{y}_i$) is shown in Figure~\ref{mkDiagnosticEvsAbsF}. This plot is helpful to check the condition that the variance of the residuals is approximately constant. We don't see any obvious deviations from constant variance in this example.

\begin{figure}
  \centering
  \Figures{0.7}
      {marioKartDiagnostics}
      {mkDiagnosticEvsAbsF}
  \caption{Comparing the absolute value of the residuals against the fitted values ($\hat{y}_i$) is helpful in identifying deviations from the constant variance assumption.}
  \label{mkDiagnosticEvsAbsF}
\end{figure}

\item[Residuals in order of their data collection.] A plot of the residuals in the order their corresponding auctions were observed is shown in Figure~\ref{mkDiagnosticInOrder}. Such a plot is helpful in identifying any connection between cases that are close to one another, e.g. we could look for declining prices over time or if there was a time of the day when auctions tended to fetch a higher price. Here we see no structure that indicates a problem.\footnote{An especially rigorous check would use \term{time series} methods. For instance, we could check whether consecutive residuals are correlated. Doing so with these residuals yields no statistically significant correlations.}

\begin{figure}[h]
  \centering
  \Figures{0.72}{marioKartDiagnostics}{mkDiagnosticInOrder}
  \caption{Plotting residuals in the order that their
      corresponding observations were collected helps
      identify connections between successive observations.
      If it seems that consecutive observations tend to be
      close to each other, this indicates the independence
      assumption of the observations would fail.}
  \label{mkDiagnosticInOrder}
\end{figure}

\item[Residuals against each predictor variable.] We consider a plot of the residuals against the \var{cond\us{}new} variable, the residuals against the \var{stock\us{}photo} variable, and the residuals against the \var{wheels} variable. These plots are shown in Figure~\ref{mkDiagnosticEvsVariables}. For the two-level condition variable, we are guaranteed not to see any remaining trend, and instead we are checking that the variability doesn't fluctuate across groups, which it does not. However, looking at the stock photo variable, we find that there is some difference in the variability of the residuals in the two groups. Additionally, when we consider the residuals against the \var{wheels} variable, we see some possible structure. There appears to be curvature in the residuals, indicating the relationship is probably not linear.

\begin{figure}
  \centering
  \Figures{}{marioKartDiagnostics}{mkDiagnosticEvsVariables}
  \caption{For the condition and stock photo variables,
      we check for differences in the distribution shape
      or variability of the residuals.
      In the case of the stock photos variable, we see
      a little less variability in the unique photo group
      than the stock photo group.
      For numerical predictors, we also check for trends
      or other structure.
      We see some slight bowing in the residuals against
      the \var{wheels} variable in the bottom plot.}
  \label{mkDiagnosticEvsVariables}
\end{figure}

\end{description}

It is necessary to summarize diagnostics for any model fit. If the diagnostics support the model assumptions, this would improve credibility in the findings. If the diagnostic assessment shows remaining underlying structure in the residuals, we should try to adjust the model to account for that structure. If we are unable to do so, we may still report the model but must also note its shortcomings. In the case of the auction data, we report that there appears to be non-constant variance in the stock photo variable and that there may be a nonlinear relationship between the total price and the number of wheels included for an auction. This information would be important to buyers and sellers who may review the analysis, and omitting this information could be a setback to the very people who the model might assist.

\begin{onebox}{``All models are wrong, but some are useful''~~~-George E.P. Box}
The truth is that no model is perfect. However, even imperfect models can be useful. Reporting a flawed model can be reasonable so long as we are clear and report the model's shortcomings.
\end{onebox}

Don't report results when assumptions are grossly violated.
While there is a little leeway in model assumptions, don't go too far. If model assumptions are very clearly violated, consider a new model, even if it means learning more statistical methods or hiring someone who can help.

\begin{onebox}{Confidence intervals in multiple regression}
\index{confidence interval}%
Confidence intervals for coefficients in multiple regression can be computed using the same formula as in the single predictor model:
\begin{align*}
b_i \ \pm\ t_{df}^{\star}SE_{b_{i}}
\end{align*}
where $t_{df}^{\star}$ is the appropriate $t$-value corresponding to the confidence level and model degrees of freedom, $df=n-k-1$.
\index{multiple regression!model assumptions|)}
\index{data!mario\_kart|)}
\index{multiple regression|)}
\end{onebox}


%__________________
\section{Introduction to logistic regression}
\label{logisticRegression}

\Comment{We will be reaching out to groups that work
    on the topic of discrimination to provide feedback
    on the presentation of the material in this section.
    This current version is still very much only a draft.}

\index{logistic regression|(}

In this section we introduce \term{logistic regression}
as a tool for building models when there is a categorical
response variable with two levels, e.g. yes and no.
Logistic regression is a type of
\term{generalized linear model} (GLM) for response variables
where regular multiple regression does not work very well.
In particular, the response variable in these settings often
takes a form where residuals look completely different from
the normal distribution.

GLMs can be thought of as a two-stage modeling approach.
We first model the response variable using a probability
distribution, such as the binomial or Poisson distribution.
Second, we model the parameter of the distribution using
a collection of predictors and a special form of multiple
regression.
Ultimately, the application of a GLM will feel very similar
to multiple regression, even if some of the details are
different.

%In Section~\ref{logisticRegression} we will revisit the \data{email} data set from Chapter~\ref{introductionToData}. These emails were collected from a single email account, and we will work on developing a basic spam filter using these data. The response variable, \var{spam}, has been encoded to take value~0 when a message is not spam and~1 when it is spam. Our task will be to build an appropriate model that classifies messages as spam or not spam using email characteristics coded as predictor variables. While this model will not be the same as those used in large-scale spam filters, it shares many of the same features. 

\subsection{Resume data}

\index{data!resume|(}

\newcommand{\resN}{4870}
\newcommand{\resCallbackProp}{0.0805}
\newcommand{\resCallbackPerc}{8.05\%}
\newcommand{\resNumPred}{8}
\newcommand{\resUniqueNames}{36}
\newcommand{\resHonorsInt}{-2.4998}
\newcommand{\resHonorsCoef}{0.8668}
\newcommand{\resHonorsIntPlusCoef}{-1.6330}
\newcommand{\resHonorsCoefSE}{0.1776}
\newcommand{\resHonorsCoefZ}{4.88}
\newcommand{\resHonorsProb}{0.163}
\newcommand{\resHonorsPerc}{16.3\%}
\newcommand{\resHonorsNotProb}{0.076}
\newcommand{\resHonorsNotPerc}{7.6\%}

We will consider experiment data from a study that sought
to understand the effect of race and sex on job application
callback rates.
To evaluate which factors were important,
job postings were identified in Boston and Chicago
for the study,
and researchers created many fake resumes to send off
to these jobs to see which would elicit a callback.
The researchers enumerated important characteristics,
such as years of
experience and education details, and they used these
characteristics to randomly generate the resumes.
Finally, they randomly assigned a name to each resume,
where the name would imply the applicant's sex and race.
The first names used in this experiment were selected so that
the names would predominantly be
recognized as belonging to black or white individuals.
For example, Lakisha was a name that their survey indicated
would be interpreted as a black woman, while Greg was a name
that would generally be interpreted to be associated with
a white male.
You can find the full set of names they used in
Figure~\ref{resumeFirstName}.

\begin{figure}[h]
\centering\small
\begin{tabular}{lll c lll c lll}
  \cline{1-3} \cline{5-7} \cline{9-11}
  first\us{}name & race & sex
      & \ \hspace{2mm}\ &
      first\us{}name & race & sex
      & \ \hspace{2mm}\ &
      first\us{}name & race & sex
      \\
  \cline{1-3} \cline{5-7} \cline{9-11}
  Aisha & black & female &&
      Hakim & black & male &&
      Laurie & white & female \\
  Allison & white & female &&
      Jamal & black & male &&
      Leroy & black & male \\
  Anne & white & female &&
      Jay & white & male &&
      Matthew & white & male \\
  Brad & white & male &&
      Jermaine & black & male &&
      Meredith & white & female \\
  Brendan & white & male &&
      Jill & white & female &&
      Neil & white & male \\
  Brett & white & male &&
      Kareem & black & male &&
      Rasheed & black & male \\
  Carrie & white & female &&
      Keisha & black & female &&
      Sarah & white & female \\
  Darnell & black & male &&
      Kenya & black & female &&
      Tamika & black & female \\
  Ebony & black & female &&
      Kristen & white & female &&
      Tanisha & black & female \\
  Emily & white & female &&
      Lakisha & black & female &&
      Todd & white & male \\
  Geoffrey & white & male &&
      Latonya & black & female &&
      Tremayne & black & male \\
  Greg & white & male &&
      Latoya & black & female &&
      Tyrone & black & male \\
  \cline{1-3} \cline{5-7} \cline{9-11}
\end{tabular}
\caption{List of all \resUniqueNames{} unique names along
    with the commonly inferred race and sex associated
    with these names.}
\label{resumeFirstName}
\end{figure}
% library(openintro); library(xtable); vars <- c("firstname", "race", "gender"); d <- resume[, vars]; names(d)[1] <- "first_name"; d <- unique(d); d <- d[order(d$first_name), ]; rownames(d) <- NULL; d. <- cbind(d[1:12, ], d[13:24, ], d[25:36, ]); xtable(d.)

The response variable of interest is whether or not there
was a callback from the employer for the applicant,
and there were \resNumPred{} attributes that
were randomly assigned that we'll consider,
with special interest in the race and sex variables.
Race and sex are \term{protected classes} in the
United States, meaning they are not legally permitted
factors for hiring or employment decisions.
The full set of attributes considered is provided in
Figure~\ref{resumeVariables}.

\begin{figure}[h]
\centering\small
\begin{tabular}{lp{112mm}}
\hline
{\bf variable} & {\bf description} \\
\hline
\var{callback} &
    Specifies whether the employer called the applicant
    following submission of the application for the job. \\
%\var{first\us{}name} &
%    First name of the applicant that is listed on the resume. \\
\var{job\us{}city} &
    City where the job was located: Boston or Chicago.\\
%\var{job\us{}industry} &
%    The job industry, e.g. manufacturing or transportation,
%    for the job listing. \\
%\var{job\us{}type} &
%    The type of job, e.g. supervisor or sales representative,
%    for the job listing. \\
%\var{job\us{}req} &
%    An indicator for if there were any job requirements listed
%    in the job listing. \\
\var{college\us{}degree} &
    An indicator for whether the resume listed a college degree. \\
\var{years\us{}experience} &
    Number of years of experience listed on the resume. \\
\var{honors} &
    Indicator for the resume listing some sort of honors,
    e.g.~employee of the month. \\
\var{military} &
    Indicator for if the resume listed any military experience. \\
\var{email\us{}address} &
    Indicator for if the resume listed an email address for
    the applicant. \\
\var{race} &
    Race of the applicant, implied by their first name
    listed on the resume. \\
\var{sex} &
    Sex of the applicant (limited to only \resp{male}
    and \resp{female} in this study),
    implied by the first name listed on the resume. \\
\hline
\end{tabular}
\caption{Descriptions for the \var{callback} variable
    along with \resNumPred{} other variables
    in the \data{resume} data set.
    Many of the variables are
    indicator\index{indicator variable} variables,
    meaning they take the value 1 if the specified
    characteristic is present and 0 otherwise.}
\label{resumeVariables}
\end{figure}

All of the attributes listed on each resume were
randomly assigned.
This means that no attributes that might be favorable
or detrimental to employment would favor one demographic
over another on these resumes.
Importantly, due to the experimental nature of this study,
we can infer causation between these variables and the
callback rate, if the variable is statistically significant.
Our analysis will allow us to compare the practical
importance of each of the variables relative to each other.




\subsection{Modeling the probability of an event}
\label{modelingTheProbabilityOfAnEvent}

Logistic regression is a generalized linear model where
the outcome is a two-level categorical variable.
The outcome, $Y_i$, takes the value 1
(in our application, this represents a callback
for the resume)
with probability $p_i$
and the value 0 with probability $1 - p_i$.
Because each observation has a slightly different
context, e.g. different education level or a different
number of years of experience, the probability $p_i$
will differ for each observation.
Ultimately, it is this probability that we model
in relation to the predictor variables:
we will examine which resume characteristics
correspond to higher or lower callback rates.

\begin{onebox}{Notation for a logistic regression model}
The outcome variable for a GLM is denoted by $Y_i$,
where the index $i$ is used to represent observation $i$.
In the resume application, $Y_i$ will be used to represent
whether resume $i$ received a callback ($Y_i=1$)
or not ($Y_i=0$). \vspace{3mm}

The predictor variables are represented as follows:
$x_{1,i}$ is the value of variable 1 for observation $i$,
$x_{2,i}$ is the value of variable 2 for observation $i$,
and so on.
\end{onebox}

The logistic regression model relates the probability
a resume would receive a callback ($p_i$) to the predictors
$x_{1,i}$, $x_{2,i}$, ..., $x_{k,i}$
through a framework much like that of multiple regression:
\begin{align}
transformation(p_{i})
  = \beta_0 +
      \beta_1x_{1,i} +
      \beta_2 x_{2,i} +
      \cdots +
      \beta_k x_{k,i}
\label{linkTransformationEquation}
\end{align}
We want to choose a transformation in the equation
that makes practical and mathematical sense.
For example, we want a transformation that makes
the range of possibilities on the left hand side
of the equation equal to the range of possibilities
for the right hand side;
if there was no transformation for this equation,
the left hand side could only take values between 0 and 1,
but the right hand side could take values outside of this
range.
A common transformation for $p_i$ is the \term{logit transformation}, which may be written as
\begin{align*}
logit(p_i) = \log_{e}\left( \frac{p_i}{1-p_i} \right)
\end{align*}
The logit transformation is shown in
Figure~\ref{logitTransformationFigureHoriz}.
Below, we rewrite the equation relating $Y_i$ to its
predictors using the logit transformation of $p_i$:
\begin{align*}
\log_{e}\left( \frac{p_i}{1-p_i} \right)
  = \beta_0 +
      \beta_1 x_{1,i} +
      \beta_2 x_{2,i} +
      \cdots +
      \beta_k x_{k,i}
\end{align*}
In our resume example, there are \resNumPred{} predictor
variables, so $k = \resNumPred{}$.
While the precise choice of a logit function isn't intuitive,
it is based on theory that underpins generalized linear models,
which is beyond the scope of this book.
Fortunately, once we fit a model using software,
it will start to feel like we're back in the
multiple regression context, even if the
interpretation of the coefficients is more complex.

\begin{figure}
  \centering
  \Figure{}{logitTransformationFigureHoriz}
  \caption{Values of $p_i$ against values of $logit(p_i)$.}
  \label{logitTransformationFigureHoriz}
\end{figure}

\begin{examplewrap}
\begin{nexample}{We start by fitting a model with a single
    predictor: \var{honors}.
    This variable indicates whether the applicant had any
    type of honors listed on their resume,
    such as employee of the month.
    The following logistic regression model was fit using
    statistical software:
    \begin{align*}
    \log\left( \frac{p_i}{1-p_i} \right)
      = \resHonorsInt{} +
          \resHonorsCoef{} \times\text{\var{honors}}
    \end{align*}
    %library(openintro); m <- glm(received_callback ~ honors, data = resume, family=binomial); summary(m); co <- round(m$coefficients, 4); a <- exp(co["(Intercept)"]); a/(1+a); a <- exp(sum(co)); a/(1+a)
    (a) If a resume is randomly selected from the study
    and it does not have any honors listed,
    what is the probability resulted in a callback?
    
    (b) What would the probability be if the resume did
    list some honors?}
    \label{logisticExampleWithHonors}%
  (a) If a randomly chosen resume from those sent out is considered,
  and it does not list honors, then \var{honors} takes
  value~0 and the right side of the model equation equals
  \resHonorsInt{}.
  Solving for $p_i$:
  $\frac{e^{\resHonorsInt{}}}{1 + e^{\resHonorsInt{}}}
      = \resHonorsNotProb{}$.
  Just as we labeled a fitted value of $y_i$ with a ``hat''
  in single-variable and multiple regression, we do the same
  for this probability: $\hat{p}_i = \resHonorsNotProb{}$.

  (b) If the resume had listed some honors,
  then the right side of the model equation is
  $\resHonorsInt{} + \resHonorsCoef{} \times 1
      = \resHonorsIntPlusCoef{}$,
  which corresponds to a probability
  $\hat{p}_i = \resHonorsProb{}$.

  Notice that we could examine \resHonorsInt{} and
  \resHonorsIntPlusCoef{} in
  Figure~\ref{logitTransformationFigureHoriz}
  to estimate the probability before formally calculating
  the value.
\end{nexample}
\end{examplewrap}

To convert from values on the logistic regression scale
(e.g. \resHonorsInt{} and \resHonorsIntPlusCoef{} in
Example~\ref{logisticExampleWithHonors}),
use the following formula, which is the result
of solving for $p_i$ in the regression model:
\newcommand{\exponentialToSolveForPi}
    {e^{\beta_0 + \beta_1 x_{1,i}+\cdots+\beta_k x_{k,i}}}%
\begin{align*}
p_i = \frac{\exponentialToSolveForPi{}}
    {\ 1\ \ +\ \ \exponentialToSolveForPi{}\ }
\end{align*}
As with most applied data problems,
we substitute the point estimates for the parameters
(the $\beta_i$) so that we can make use of this formula.
In Example~\ref{logisticExampleWithHonors},
the probabilities were calculated as
\begin{align*}
&\frac{\ e^{\resHonorsInt{}}\ }
    {\ 1\ +\ e^{\resHonorsInt{}}\ }
  = \resHonorsNotProb{} &&
\frac{\ e^{\resHonorsInt{} + \resHonorsCoef{}}\ }
    {\ 1\ +\ e^{\resHonorsInt{} + \resHonorsCoef{}}\ }
  = \resHonorsProb{}
\end{align*}
While knowing whether a resume listed honors provides
some signal when predicting whether or not the employer
would call, we would like to account for many different
variables at once to understand how each of the different
resume characteristics affected the chance of a callback.


\subsection{Building the logistic model with many variables}

We used statistical software to fit the logistic regression
model with all \resNumPred{} predictors described in
Figure~\ref{resumeVariables}.
Like multiple regression, the result may be presented
in a summary table, which is shown in
Figure~\ref{resumeLogisticModelResults}.
The structure of this table is almost identical to that
of multiple regression;
the only notable difference is that the p-values are
calculated using the normal distribution rather than
the $t$-distribution.

\begin{figure}[ht]
\centering
\begin{tabular}{l rrrr}
  \hline
  \vspace{-3.7mm} & & & & \\
  & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\
  \hline
  \vspace{-3.8mm} & & & & \\
  (Intercept) & -2.6632 & 0.1820 & -14.64 & $<$0.0001 \\
  job\us{}city\lmlevel{Chicago} &
      -0.4403 & 0.1142 & -3.85 & 0.0001 \\
  college\us{}degree & -0.0666 & 0.1211 & -0.55 & 0.5821 \\
  years\us{}experience & 0.0200 & 0.0102 & 1.96 & 0.0503 \\
  honors & 0.7694 & 0.1858 & 4.14 & $<$0.0001 \\
  military & -0.3422 & 0.2157 & -1.59 & 0.1127 \\
  email\us{}address & 0.2183 & 0.1133 & 1.93 & 0.0541 \\
  race\lmlevel{white} & 0.4424 & 0.1080 & 4.10 & $<$0.0001 \\
  sex\lmlevel{male} & -0.1818 & 0.1376 & -1.32 & 0.1863 \\
  \hline
\end{tabular}
\caption{Summary table for the full logistic regression model
    for the resume callback example.}
\label{resumeLogisticModelResults}
\end{figure}
% library(openintro); library(dplyr); a <- resume; d <- data.frame(callback = a$received_callback, job_city = a$job_city, college_degree = a$college_degree, years_experience = a$years_experience, honors = a$honors, military = a$military, email_address = a$has_email_address, race = a$race, gender = ifelse(a$gender == "m", "male", "female"))
% job_industry = a$job_industry, job_type = a$job_type, 
% m <- glm(callback ~ job_city + college_degree + years_experience + honors + military + email_address + race + gender, data = d, family = binomial); summary(m); xtable(m)
\newcommand{\resRaceWhiteCoef}{0.4424}

Just like multiple regression, we could trim some variables
from the model.
Here we'll use a statistic called
\term{Akaike information criterion (AIC)},
which is an analog to how we used adjusted R-squared
in multiple regression,
and we look for models with a lower AIC
through a backward elimination strategy.
After using this criteria, the \var{college\us{}degree}
variable is eliminated, giving the smaller model summarized
in Figure~\ref{resumeLogisticReducedModel},
which is what we'll rely on for the remainder
of this section.
\Comment{Do we want to discuss that one variable dropping out more?}

\begin{figure}[ht]
\centering
\begin{tabular}{l rrrr}
  \hline
  \vspace{-3.7mm} & & & & \\
  & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
  \vspace{-3.8mm} & & & & \\
  (Intercept) & -2.7162 & 0.1551 & -17.51 & $<$0.0001 \\ 
  job\us{}city\lmlevel{Chicago} &
      -0.4364 & 0.1141 & -3.83 & 0.0001 \\ 
  years\us{}experience & 0.0206 & 0.0102 & 2.02 & 0.0430 \\ 
  honors & 0.7634 & 0.1852 & 4.12 & $<$0.0001 \\ 
  military & -0.3443 & 0.2157 & -1.60 & 0.1105 \\ 
  email\us{}address & 0.2221 & 0.1130 & 1.97 & 0.0494 \\ 
  race\lmlevel{white} & 0.4429 & 0.1080 & 4.10 & $<$0.0001 \\ 
  sex\lmlevel{male} & -0.1959 & 0.1352 & -1.45 & 0.1473 \\ 
\hline
\end{tabular}
\caption{Summary table for the logistic regression model
    for the resume callback example, where variable selection
    has been performed using AIC.}
\label{resumeLogisticReducedModel}
\end{figure}
% # Run code for table above first
% % m. <- step(m); summary(m.); xtable(m.)
\newcommand{\resRaceWhiteCoefReduced}{0.4429}

\begin{examplewrap}
\begin{nexample}{The \var{race} variable had taken
    only two levels: \resp{black} and \resp{white}.
    Based on the model results, was race a meaningful
    factor for if a prospective employer would
    call back?}
  We see that the p-value for this coefficient is very
  small (essentially zero), which implies that race
  played a statistically significant role in whether
  a candidate received a callback.
  Additionally, we see that the coefficient shown
  corresponds to the level of \resp{white},
  and it is positive,
  which implies that resumes from individuals with
  names typically associated with white individuals
  tended to receive more callbacks than those
  typically associated with black individuals.
\end{nexample}
\end{examplewrap}

The data provide very strong evidence of racial bias
by prospective employers that favors resumes where the
first name is typically interpreted to be white.
The coefficient shown in the model for the
race variable reflects a positive gain in callback
rate for resumes where the candidate's first name
implied they were white.
%We, the authors, found this conclusion saddening,
%though not surprising.
%It is also important to consider that this data only
%highlights one stage of racial bias in employment --
%when someone is trying to get hired --
%and it does not consider racial bias during employment.
%It does not scratch the surface of racial bias
%for individuals who are hired.

\begin{examplewrap}
\begin{nexample}{Compare the coefficient of the
    \var{racewhite} row in the full model in
    Figure~\ref{resumeLogisticModelResults},
    versus the summary of the reduced model in
    Figure~\ref{resumeLogisticReducedModel}.
    Why are the two estimated coefficients so
    close to each other?}
  The estimates are only very slightly different:
  \resRaceWhiteCoefReduced{} vs \resRaceWhiteCoef{}.
  We earlier discussed how the implied race on the resume
  was randomized and this variable is independent of
  other predictors.
  This means that the estimated effect will be virtually
  unchanged even after we add or remove other variables
  from the model.
  This property is the product of thoughtful experiment
  design by this study's researchers.
\end{nexample}
\end{examplewrap}

The predictors in this experiment were thoughtfully
laid out so that they were independent,
which aligned with the motivation of the study to tease
out which effects were important to getting a callback.
When a data set doesn't come from a thoughtfully designed
experiment -- for instance, maybe it is observational data --
it's common for point estimates to change a little,
and sometimes a lot, depending on which other
variables are included in the model.
This is usually due to colinearity in the predictor variables.
\Comment{Revisit the end of this paragraph,
  e.g. if removing the Ebay auction example.}
We previously saw this in the Ebay auction example when
we compared the coefficient of \var{cond\us{}new} in a
single-variable model and the corresponding coefficient
in the multiple regression model when including three
additional variables (see
Sections~\ref{ind_and_cat_vars_as_predictors}
and~\ref{includingAndAssessingManyVariablesInAModel}).

\begin{examplewrap}
\begin{nexample}{Use the model summarized in
    Figure~\ref{resumeLogisticReducedModel}
    to estimate the probability
    of receiving a callback for a job in Chicago
    where the candidate lists 14 years experience,
    no honors,
    no military experience,
    includes an email address,
    and has a first name that implies they are a white male.}
  \label{exampleForResumeAndWhiteQuantified}%
  We can start by writing out the equation using indicator
  functions and coefficients from the model, then we can
  add in the corresponding values of each variable for this
  individual:
  \begin{align*}
  log\left(\frac{p}{1 - p}\right)
    &= - 2.7162
        - 0.4364 \times 1_{Chicago}
        + 0.0206 \times \text{(years experience)}
        + 0.7634 \times 1_{honors} \\
      &\qquad
          - 0.3443 \times 1_{military}
          + 0.2221 \times 1_{email}
          + 0.4424 \times 1_{white}
          - 0.1959 \times 1_{male} \\
    &= - 2.7162
        - 0.4364 \times 1
        + 0.0206 \times 14
        + 0.7634 \times 0 \\
      &\qquad
          - 0.3443 \times 0
          + 0.2221 \times 1
          + 0.4424 \times 1
          - 0.1959 \times 1 \\
    &= - 2.3956
  \end{align*}
  We can now back-solve for $p$:
  the chance such an individual will receive
  a callback is about~8.35\%.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{Compute the probability of a callback
    for an individual with a name commonly inferred
    to be from a black male but who otherwise
    has the same characteristics as the one described
    in Example~\ref{exampleForResumeAndWhiteQuantified}.}
  \index{exampleForResumeAndBlackQuantified}%
  We can complete the same steps for an individual
  with the same characteristics who is black,
  where the only difference in the calculation is that
  the indicator variable
  $1_{white}$ will take a value of \resp{0}.
  Doing so yields a probability of 0.0553.
  Let's compare the results with those of
  Example~\ref{exampleForResumeAndWhiteQuantified}.

  In practical terms, such an individual perceived
  as white based on their first name would need to
  apply to $\frac{1}{0.0835} \approx 12$ jobs on average
  to receive a callback,
  while an individual perceived as black based on their
  first name would need
  to apply to $\frac{1}{0.0553} \approx 18$ jobs on average
  to receive a callback.
  Applicants who are perceived as
  black needing to apply to about 50\% more employers
  to receive a callback than someone who is perceived
  as white based on their first name for jobs like
  those in the study.
\end{nexample}
\end{examplewrap}

What we've quantified in this section is alarming and disturbing.
However, one aspect that makes this racial bias so difficult to
address is that the experiment, as well-designed as it is,
cannot send us much signal about which employers are
discriminating.
It is only possible to say that discrimination is happening,
even if we cannot say which particular callbacks
-- or non-callbacks -- represent discrimination.
Finding strong evidence of racism for individual cases is
a persistent challenge in enforcing anti-discrimination laws.
%For observational data on racial discrimination,
%there are even more challenges:
%some variables may be correlated with race
%or there may be potential confounding variables that
%cannot reasonably be modeled,
%making the challenges even more profound in reliably
%identifying racism.



\subsection{Diagnostics for the callback rate model}
\label{logistic_regr_diagnostics_subsection}

\begin{onebox}{Logistic regression conditions}
There are two key conditions for fitting a logistic regression model:\vspace{-1mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item
    Each outcome $Y_i$ is independent of the other outcomes.
\item
    Each predictor $x_i$ is linearly related to logit$(p_i)$
    if all other predictors are held constant.
\end{enumerate}
\end{onebox}

The first logistic regression model assumption
-- independence of the outcomes --
is reasonable for the experiment since characteristics
of resumes were randomly assigned to the resumes that
were sent out.
%This is further discussed in Appendix~\ref{}.

The second condition of the logistic regression model is
not easily checked without a fairly sizable amount of data.
Luckily, we have \resN{} resume submissions in the data set!
Let's first visualize these data by plotting the true
classification of the resumes against the model's fitted
probabilities, as shown in
Figure~\ref{logisticModelPredict}.
%The vast majority of emails (spam or not) still have fitted probabilities below 0.5.

\begin{figure}[h]
  \centering
  \Figures{0.95}{logisticModel}{logisticModelPredict}
  \caption{The predicted probability that each of the
      \resN{} resumes results in a callback.
      \hiddenterm{Noise}
      (small, random vertical shifts) have been added
      to each point so points with nearly identical
      values aren't plotted exactly on top of one another.}
  \label{logisticModelPredict}
\end{figure}

%The probabilities predicted by the model fall between
%4.3\% and 29.9\%.
We'd like to assess the quality of the model.
For example, we might ask:
if we look at resumes that we modeled as having
a 10\% chance of getting a callback, do we find
about 10\% of them actually receive a callback?
To help us out, we'll borrow an advanced statistical
method called \term{natural splines} that estimates
the local probability over the region 0.04 to 0.30,
which is the range of the predicted probabilities.
All you need to know about natural splines to understand
what we are doing is that they are used to fit flexible
lines rather than straight lines.

The curve fit using natural splines is shown in
Figure~\ref{logisticModelSpline} as a solid black line.
If the logistic model fits well, the curve should closely
follow the dashed $y = x$ line.
We have added shading to represent the confidence bound for
the curved line to clarify what fluctuations might plausibly
be due to chance.
The dashed line generally stays within the error bound
of the solid curve, suggesting the fit is reasonable.

\begin{figure}
  \centering
  \Figures{0.95}{logisticModel}{logisticModelSpline}
  \caption{The dashed line is within the confidence bound
       of the smoothed line, suggesting the logistic fit is
       reasonable.}
  \label{logisticModelSpline}
\end{figure}

Additional diagnostics may be created that are similar to those
featured in Section~\ref{multipleRegressionModelAssumptions}.
For instance, we could compute residuals as
the observed outcome minus the expected outcome
($e_i = Y_i - \hat{p}_i$),
and then we could create plots of these residuals
against each predictor.
We might also create a smoothed average like that in
Figure~\ref{logisticModelSpline} to better understand
deviations.

\index{data!resume|)}
\index{logistic regression|)}



\subsection{The mathematics behind discrimination}

\Comment{This subsection is tentatively added due to the
    importance of discussing racial discrimination.
    We would like feedback on whether it seems appropriate
    to retain this subsection, or whether it should be moved
    to a blog post or other place.
    No matter where it goes, we will have it carefully
    reviewed by an outside group that specializes in the
    topic of discrimination.}

\index{discrimination|(}

%Discrimination is an incredibly important and complex societal issue, and this study only examined discrimination in a single aspect
Any form of discrimination is concerning,
and this is why we decided it was so important to discuss
this topic using data.
This study also only examined discrimination in a
single aspect: whether a prospective employer would
call a candidate who submitted their resume.
There was a 50\% higher barrier for resumes simply when
the candidate had a first name that was perceived to be
from a black individual.
It's unlikely that discrimination would stop there.

Despite these considerations, we've observed calls for
considerations around \emph{reverse discrimination},
\index{reverse discrimination}%
which is the notion that individuals from minority groups
get special treatment because of their minority status,
translating to discrimination against a non-minority group.
Such discrimination is likely real in some situations.
Yet, the impact appears to be so much smaller than the
discrimination experienced by minorities... \emph{why?}

\begin{examplewrap}
\begin{nexample}{Let's consider a sex-imbalanced
    company that consists of 20\% women
    and 80\% men,\footnotemark and we'll suppose that the
    company is very large, consisting of perhaps
    20,000 employees.
    Suppose when someone goes up for promotion at this
    company, 5~of their colleagues are randomly chosen
    to provide feedback on their work.
    \exspace{}

    Now let's imagine that 10\% of the people in the
    company are prejudiced against the other sex.
    That is, 10\% of men are prejudiced against women,
    and similarly, 10\% of women are prejudiced against men.
    \exspace{}
    
    Who is discriminated against more at the company,
    men or women?}
  \label{sex_imbalance_leads_to_discrimination}%
  Let's suppose we took 100 men who have gone up for
  promotion in the past few years.
  For these men, $5 \times 100 = 500$ random colleagues
  will be tapped for their feedback, of which,
  about 20\% will be women (100 women).
  Of these 100 women, 10 are expected to be biased
  against the man they are reviewing.
  Then, of the 500 colleagues reviewing them,
  men will experience
  discrimination by about 2\% of their colleagues when
  they go up for promotion.

  Let's do a similar calculation for 100 women
  who have gone up for promotion in the last few years.
  They will also have 500 random colleagues providing
  feedback, of which about 400 (80\%) will be men.
  Of these 400 men, about 40 (10\%) hold a bias against
  women.
  Of the 500 colleagues providing feedback on the
  promotion packet for these women, 8\% of the
  colleagues hold a bias against the women.
\end{nexample}
\end{examplewrap}
\footnotetext{A more thoughtful example would include
    non-binary individuals.}

Example~\ref{sex_imbalance_leads_to_discrimination}
highlights something profound:
even if each demographic has the same degree of prejudice
against the other demographic, minority groups
experience those effects more frequently.
Additionally, if we would complete a handful of examples
like the one above with different numbers,
we'd learn that the greater the imbalance
in the population groups, the more the minority
is disproportionately impacted.
For example, if a proportion $p$ of a company are
women and the rest of the company consists rest men,
then the ratio of rates of discrimination against women
vs men would be given by $\frac{1 - p}{p}$;
this ratio is always greater than 1 when $p < 0.5$.
That is, whenever individuals are equally likely to
be prejudiced, the minority group would always encounter
more discrimination, and the degree of that discrimination
will be larger the greater the imbalance in the population.

The 8\%-to-2\% is a direct result of the 80\%-to-20\% ratio
in Example~\ref{sex_imbalance_leads_to_discrimination}.
More generally, if 

No discrimination has a place in our society,
be it discrimination against a minority group
or a majority group.
Yet we cannot deny the mathematics behind
discrimination: minority groups are much more
prone to severe effects from discrimination
than majority groups.
While we will stand up to discrimination in all
its forms, we recognize that the biggest problems
we face today is discrimination against minorities.

We close this book on this serious topic,
and we hope it inspires you to think about
the power of reasoning with data.
Whether it is with a formal statistical model
or by using critical thinking skills to structure
a problem, we hope the ideas you have learned will
help you do more and do better in life.

\index{discrimination|)}
